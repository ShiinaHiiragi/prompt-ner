{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded succeed\n"
     ]
    }
   ],
   "source": [
    "from utils.saver import tokenizer_loader, model_loader\n",
    "from operators.CONLLReader import CONLLReader\n",
    "from operators.NERDataset import NERDataset\n",
    "\n",
    "from PromptWeaver import BartPromptOperator\n",
    "from transformers import BertTokenizer, BartForConditionalGeneration\n",
    "\n",
    "DATASET_NAME = \"msra.min\"\n",
    "dataset = NERDataset(\n",
    "    reader=CONLLReader(f\"./data/{DATASET_NAME}.test\"),\n",
    "    tokenizer=tokenizer_loader(BertTokenizer, \"fnlp/bart-base-chinese\")\n",
    ")\n",
    "model = model_loader(BartForConditionalGeneration, \"fine-tune/prompt-bart\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from utils.constants import DEVICE\n",
    "\n",
    "GRAM = 4\n",
    "model.to(DEVICE)\n",
    "\n",
    "def calc_labels_entity(dataset):\n",
    "    labels = list(set(\n",
    "        map(\n",
    "            lambda item: item[2:],\n",
    "            filter(\n",
    "                lambda item: item != \"O\",\n",
    "                dataset.id_label\n",
    "            )\n",
    "        )\n",
    "    ))\n",
    "\n",
    "    return { item: BartPromptOperator.LABEL_ENTITY[item] for item in labels }\n",
    "\n",
    "def generate_template(sentence_str, start_point, part_labels_entity):\n",
    "    result = []\n",
    "    for span_size in range(1, GRAM + 1):\n",
    "        span = sentence_str[start_point:start_point + span_size]\n",
    "        result.append(BartPromptOperator.NEGATIVE_TEMPLATE.format(candidate_span=span))\n",
    "        for entity in part_labels_entity.keys():\n",
    "            result.append(BartPromptOperator.POSITIVE_TEMPLATE.format(\n",
    "                candidate_span=span,\n",
    "                entity_type=part_labels_entity[entity]\n",
    "            ))\n",
    "\n",
    "    def find_tag(index):\n",
    "        part_labels = list(part_labels_entity.keys())\n",
    "        part_labels.insert(0, \"O\")\n",
    "        label_size = len(part_labels)\n",
    "\n",
    "        span_size = index // label_size + 1\n",
    "        span_type = index % label_size\n",
    "        return span_size, part_labels[span_type]\n",
    "\n",
    "    return result, find_tag\n",
    "\n",
    "def calc_max_possible(model, tokenizer, sentence_str, templates):\n",
    "    batch_size = len(templates)\n",
    "    inputs_id = tokenizer([sentence_str] * batch_size, return_tensors=\"pt\")[\"input_ids\"]\n",
    "    outputs = tokenizer(templates, return_tensors=\"pt\", padding=True)\n",
    "    outputs_id = outputs[\"input_ids\"]\n",
    "\n",
    "    outputs_id_length = torch.sum(outputs[\"attention_mask\"], axis=1) - 2\n",
    "    max_outputs_id_length = int(max(outputs_id_length))\n",
    "\n",
    "    score = [1] * batch_size\n",
    "    with torch.no_grad():\n",
    "        logits = model(input_ids=inputs_id.to(DEVICE), decoder_input_ids=outputs_id.to(DEVICE))[0]\n",
    "        for token_index in range(max_outputs_id_length):\n",
    "            single_logits = logits[:, token_index, :].softmax(dim=1).to('cpu').numpy()\n",
    "            for sentence_index in range(batch_size):\n",
    "                if token_index < outputs_id_length[sentence_index]:\n",
    "                    next_token_id = int(outputs_id[sentence_index, token_index + 1])\n",
    "                    score[sentence_index] *= single_logits[sentence_index][next_token_id]\n",
    "\n",
    "    max_score = max(score)\n",
    "    return score.index(max_score), max_score\n",
    "\n",
    "def mark_label(predict, sentence_size):\n",
    "    def is_intersect(left, right):\n",
    "        if left[1] < right[0] or left[0] > right[1]:\n",
    "            return False\n",
    "        return True\n",
    "\n",
    "    left = 0\n",
    "    while left < len(predict):\n",
    "        right = left + 1\n",
    "        while right < len(predict):\n",
    "            if is_intersect(predict[left][\"interval\"], predict[right][\"interval\"]):\n",
    "                if predict[left][\"score\"] < predict[right][\"score\"]:\n",
    "                    predict[left], predict[right] = predict[right], predict[left]\n",
    "                predict.pop(right)\n",
    "            else:\n",
    "                right += 1\n",
    "        left += 1\n",
    "\n",
    "    labels = [\"O\"] * sentence_size\n",
    "    for item in predict:\n",
    "        left, right = item[\"interval\"]\n",
    "        labels[left:right] = [f\"I-{item['type']}\"] * (right - left)\n",
    "        labels[left] = f\"B-{item['type']}\"\n",
    "\n",
    "    return labels\n",
    "\n",
    "def predict_labels(model, tokenizer, sentence_str):\n",
    "    right = len(sentence_str) - GRAM + 1\n",
    "    part_labels_entity = calc_labels_entity(dataset)\n",
    "\n",
    "    predict = []\n",
    "    for start_point in range(0, right):\n",
    "        templates, find_tag = generate_template(sentence_str, start_point, part_labels_entity)\n",
    "        max_index, max_score = calc_max_possible(model, tokenizer, sentence_str, templates)\n",
    "        span_size, span_type = find_tag(max_index)\n",
    "        if span_type != \"O\":\n",
    "            predict.append({\n",
    "                \"interval\": (start_point, start_point + span_size),\n",
    "                \"type\": span_type,\n",
    "                \"score\": max_score\n",
    "            })\n",
    "\n",
    "    labels = mark_label(predict, len(sentence_str))\n",
    "    return labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5769230769230769"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from utils.metrics import bart_calc_acc\n",
    "\n",
    "infer_labels = []\n",
    "for sentence in dataset.reader.sentences:\n",
    "    infer_label = predict_labels(model, dataset.tokenizer, \"\".join(sentence))\n",
    "    infer_labels.append(infer_label)\n",
    "\n",
    "bart_calc_acc(infer_labels, dataset.reader.labels)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
