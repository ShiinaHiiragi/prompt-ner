{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "DATASET_NAME = \"msra.min\"\n",
    "train_data = pd.read_csv(f\"./prompts/{DATASET_NAME}.train.tsv\", sep=\"\\t\").values.tolist()\n",
    "dev_data = pd.read_csv(f\"./prompts/{DATASET_NAME}.dev.tsv\", sep=\"\\t\").values.tolist()\n",
    "train_df = pd.DataFrame(train_data, columns=[\"input_text\", \"target_text\"])\n",
    "dev_df = pd.DataFrame(dev_data, columns=[\"input_text\", \"target_text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-20 12:49:41.155851: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-04-20 12:49:41.396958: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2023-04-20 12:49:41.396992: I tensorflow/compiler/xla/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2023-04-20 12:49:42.328402: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2023-04-20 12:49:42.328483: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2023-04-20 12:49:42.328491: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "from simpletransformers.seq2seq import Seq2SeqArgs\n",
    "\n",
    "model_args = Seq2SeqArgs()\n",
    "model_args.num_train_epochs = 2\n",
    "model_args.evaluate_generated_text = True\n",
    "model_args.evaluate_during_training = True\n",
    "model_args.evaluate_during_training_verbose = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'BertTokenizer'. \n",
      "The class this function is called from is 'BartTokenizer'.\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "expected str, bytes or os.PathLike object, not NoneType",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/media/ichinoe/Curriculum/Prompt NER/ner/bart_prompt.ipynb Cell 3\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/media/ichinoe/Curriculum/Prompt%20NER/ner/bart_prompt.ipynb#X12sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mutils\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mseq2seq_model\u001b[39;00m \u001b[39mimport\u001b[39;00m Seq2SeqModel\n\u001b[0;32m----> <a href='vscode-notebook-cell:/media/ichinoe/Curriculum/Prompt%20NER/ner/bart_prompt.ipynb#X12sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m model \u001b[39m=\u001b[39m Seq2SeqModel(\n\u001b[1;32m      <a href='vscode-notebook-cell:/media/ichinoe/Curriculum/Prompt%20NER/ner/bart_prompt.ipynb#X12sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m     encoder_decoder_type\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mbart\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m      <a href='vscode-notebook-cell:/media/ichinoe/Curriculum/Prompt%20NER/ner/bart_prompt.ipynb#X12sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m     encoder_decoder_name\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mfnlp/bart-base-chinese\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m      <a href='vscode-notebook-cell:/media/ichinoe/Curriculum/Prompt%20NER/ner/bart_prompt.ipynb#X12sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m     use_cuda\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m      <a href='vscode-notebook-cell:/media/ichinoe/Curriculum/Prompt%20NER/ner/bart_prompt.ipynb#X12sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m     args\u001b[39m=\u001b[39;49mmodel_args\n\u001b[1;32m      <a href='vscode-notebook-cell:/media/ichinoe/Curriculum/Prompt%20NER/ner/bart_prompt.ipynb#X12sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m )\n",
      "File \u001b[0;32m/media/ichinoe/Curriculum/Prompt NER/ner/utils/seq2seq_model.py:313\u001b[0m, in \u001b[0;36mSeq2SeqModel.__init__\u001b[0;34m(self, encoder_type, encoder_name, decoder_name, encoder_decoder_type, encoder_decoder_name, additional_special_tokens_encoder, additional_special_tokens_decoder, index_name, knowledge_dataset, index_path, dpr_ctx_encoder_model_name, rag_question_encoder_model_name, config, args, use_cuda, cuda_device, **kwargs)\u001b[0m\n\u001b[1;32m    311\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel \u001b[39m=\u001b[39m model_class\u001b[39m.\u001b[39mfrom_pretrained(encoder_decoder_name)\n\u001b[1;32m    312\u001b[0m \u001b[39mif\u001b[39;00m encoder_decoder_type \u001b[39min\u001b[39;00m [\u001b[39m\"\u001b[39m\u001b[39mbart\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mmbart\u001b[39m\u001b[39m\"\u001b[39m]:\n\u001b[0;32m--> 313\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mencoder_tokenizer \u001b[39m=\u001b[39m tokenizer_class\u001b[39m.\u001b[39;49mfrom_pretrained(\n\u001b[1;32m    314\u001b[0m         encoder_decoder_name\n\u001b[1;32m    315\u001b[0m     )\n\u001b[1;32m    316\u001b[0m \u001b[39melif\u001b[39;00m encoder_decoder_type \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mmarian\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m    317\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs\u001b[39m.\u001b[39mbase_marian_model_name:\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/transformers/tokenization_utils_base.py:1788\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *init_inputs, **kwargs)\u001b[0m\n\u001b[1;32m   1785\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1786\u001b[0m         logger\u001b[39m.\u001b[39minfo(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mloading file \u001b[39m\u001b[39m{\u001b[39;00mfile_path\u001b[39m}\u001b[39;00m\u001b[39m from cache at \u001b[39m\u001b[39m{\u001b[39;00mresolved_vocab_files[file_id]\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[0;32m-> 1788\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mcls\u001b[39;49m\u001b[39m.\u001b[39;49m_from_pretrained(\n\u001b[1;32m   1789\u001b[0m     resolved_vocab_files,\n\u001b[1;32m   1790\u001b[0m     pretrained_model_name_or_path,\n\u001b[1;32m   1791\u001b[0m     init_configuration,\n\u001b[1;32m   1792\u001b[0m     \u001b[39m*\u001b[39;49minit_inputs,\n\u001b[1;32m   1793\u001b[0m     use_auth_token\u001b[39m=\u001b[39;49muse_auth_token,\n\u001b[1;32m   1794\u001b[0m     cache_dir\u001b[39m=\u001b[39;49mcache_dir,\n\u001b[1;32m   1795\u001b[0m     \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs,\n\u001b[1;32m   1796\u001b[0m )\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/transformers/tokenization_utils_base.py:1814\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase._from_pretrained\u001b[0;34m(cls, resolved_vocab_files, pretrained_model_name_or_path, init_configuration, use_auth_token, cache_dir, *init_inputs, **kwargs)\u001b[0m\n\u001b[1;32m   1812\u001b[0m has_tokenizer_file \u001b[39m=\u001b[39m resolved_vocab_files\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mtokenizer_file\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m) \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m   1813\u001b[0m \u001b[39mif\u001b[39;00m (from_slow \u001b[39mor\u001b[39;00m \u001b[39mnot\u001b[39;00m has_tokenizer_file) \u001b[39mand\u001b[39;00m \u001b[39mcls\u001b[39m\u001b[39m.\u001b[39mslow_tokenizer_class \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m-> 1814\u001b[0m     slow_tokenizer \u001b[39m=\u001b[39m (\u001b[39mcls\u001b[39;49m\u001b[39m.\u001b[39;49mslow_tokenizer_class)\u001b[39m.\u001b[39;49m_from_pretrained(\n\u001b[1;32m   1815\u001b[0m         copy\u001b[39m.\u001b[39;49mdeepcopy(resolved_vocab_files),\n\u001b[1;32m   1816\u001b[0m         pretrained_model_name_or_path,\n\u001b[1;32m   1817\u001b[0m         copy\u001b[39m.\u001b[39;49mdeepcopy(init_configuration),\n\u001b[1;32m   1818\u001b[0m         \u001b[39m*\u001b[39;49minit_inputs,\n\u001b[1;32m   1819\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49m(copy\u001b[39m.\u001b[39;49mdeepcopy(kwargs)),\n\u001b[1;32m   1820\u001b[0m     )\n\u001b[1;32m   1821\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1822\u001b[0m     slow_tokenizer \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/transformers/tokenization_utils_base.py:1923\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase._from_pretrained\u001b[0;34m(cls, resolved_vocab_files, pretrained_model_name_or_path, init_configuration, use_auth_token, cache_dir, *init_inputs, **kwargs)\u001b[0m\n\u001b[1;32m   1921\u001b[0m \u001b[39m# Instantiate tokenizer.\u001b[39;00m\n\u001b[1;32m   1922\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 1923\u001b[0m     tokenizer \u001b[39m=\u001b[39m \u001b[39mcls\u001b[39;49m(\u001b[39m*\u001b[39;49minit_inputs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49minit_kwargs)\n\u001b[1;32m   1924\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mOSError\u001b[39;00m:\n\u001b[1;32m   1925\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mOSError\u001b[39;00m(\n\u001b[1;32m   1926\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mUnable to load vocabulary from file. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1927\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mPlease check that the provided vocabulary is accessible and not corrupted.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1928\u001b[0m     )\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/transformers/models/bart/tokenization_bart.py:220\u001b[0m, in \u001b[0;36mBartTokenizer.__init__\u001b[0;34m(self, vocab_file, merges_file, errors, bos_token, eos_token, sep_token, cls_token, unk_token, pad_token, mask_token, add_prefix_space, **kwargs)\u001b[0m\n\u001b[1;32m    205\u001b[0m mask_token \u001b[39m=\u001b[39m AddedToken(mask_token, lstrip\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, rstrip\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m) \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(mask_token, \u001b[39mstr\u001b[39m) \u001b[39melse\u001b[39;00m mask_token\n\u001b[1;32m    207\u001b[0m \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39m\u001b[39m__init__\u001b[39m(\n\u001b[1;32m    208\u001b[0m     errors\u001b[39m=\u001b[39merrors,\n\u001b[1;32m    209\u001b[0m     bos_token\u001b[39m=\u001b[39mbos_token,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    217\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs,\n\u001b[1;32m    218\u001b[0m )\n\u001b[0;32m--> 220\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39;49m(vocab_file, encoding\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mutf-8\u001b[39;49m\u001b[39m\"\u001b[39;49m) \u001b[39mas\u001b[39;00m vocab_handle:\n\u001b[1;32m    221\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mencoder \u001b[39m=\u001b[39m json\u001b[39m.\u001b[39mload(vocab_handle)\n\u001b[1;32m    222\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdecoder \u001b[39m=\u001b[39m {v: k \u001b[39mfor\u001b[39;00m k, v \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mencoder\u001b[39m.\u001b[39mitems()}\n",
      "\u001b[0;31mTypeError\u001b[0m: expected str, bytes or os.PathLike object, not NoneType"
     ]
    }
   ],
   "source": [
    "from utils.seq2seq_model import Seq2SeqModel\n",
    "\n",
    "model = Seq2SeqModel(\n",
    "    encoder_decoder_type=\"bart\",\n",
    "    encoder_decoder_name=\"fnlp/bart-base-chinese\",\n",
    "    use_cuda=False,\n",
    "    args=model_args\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.train_model(train_df, eval_data=eval_df)\n",
    "result = model.eval_model(eval_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.predict([\"他对吴宓先生的容易受愚弄不能理解，对吴先生的恋爱深不以为然，对他钟情的人尤其不满。\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
